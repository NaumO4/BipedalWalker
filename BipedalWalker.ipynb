{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BipedalWalker.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "cwiIuCWme85B",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Necessary installation for running in Google Colab"
      ]
    },
    {
      "metadata": {
        "id": "EtdPZpnCS_dq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!apt-get update\n",
        "!apt-get -qq install python-opengl -y\n",
        "!apt-get -qq -y install xvfb ffmpeg\n",
        "!pip -q install pyvirtualdisplay\n",
        "!pip -q install piglet\n",
        "!pip -q install gym[box2d]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "y7Yw5MMmfKPv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Augmented Random Search\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "il48T4lc2UWo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "from gym import wrappers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BLPLkSgekDUl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Utility class Hp with Hyperparameters"
      ]
    },
    {
      "metadata": {
        "id": "hdnA33_L3DXx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Hp():\n",
        "    # Hyperparameters\n",
        "    def __init__(self,\n",
        "                 episode_length=2000,\n",
        "                 learning_rate=0.05,\n",
        "                 num_deltas=16,\n",
        "                 num_best_deltas=16,\n",
        "                 noise=0.03,\n",
        "                 seed=1,\n",
        "                 env_name='BipedalWalker-v2',\n",
        "                 record_every=50):\n",
        "        self.episode_length = episode_length\n",
        "        self.learning_rate = learning_rate\n",
        "        self.num_deltas = num_deltas\n",
        "        self.num_best_deltas = num_best_deltas\n",
        "        assert self.num_best_deltas <= self.num_deltas\n",
        "        self.noise = noise\n",
        "        self.seed = seed\n",
        "        self.env_name = env_name\n",
        "        self.record_every = record_every\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3FMeB0ztkKs_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Utility class Normalizer that normalize data"
      ]
    },
    {
      "metadata": {
        "id": "wx9633E838cy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Normalizer():\n",
        "    # Normalizes the inputs\n",
        "    def __init__(self, nb_inputs):\n",
        "        self.n = np.zeros(nb_inputs)\n",
        "        self.mean = np.zeros(nb_inputs)\n",
        "        self.mean_diff = np.zeros(nb_inputs)\n",
        "        self.var = np.zeros(nb_inputs)\n",
        "        \n",
        "    def observe(self, x):\n",
        "        self.n += 1.0\n",
        "        last_mean = self.mean.copy()\n",
        "        self.mean += (x - self.mean) / self.n\n",
        "        self.mean_diff += (x - last_mean) * (x - self.mean)\n",
        "        self.var = (self.mean_diff / self.n).clip(min = 1e-2)\n",
        "\n",
        "    def normalize(self, inputs):\n",
        "        self.observe(inputs)\n",
        "        obs_mean = self.mean\n",
        "        obs_std = np.sqrt(self.var)\n",
        "        return (inputs - obs_mean) / obs_std\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mZpuR_R6kXfJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Agent trained in the environment"
      ]
    },
    {
      "metadata": {
        "id": "uppr2tGN4w_U",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class ArsTrainer():\n",
        "    def __init__(self, env, input_size=None, output_size=None, hp=None, normalizer=None, monitor_dir=None):\n",
        "        self.env = env\n",
        "        self.input_size = input_size or self.env.observation_space.shape[0]\n",
        "        self.output_size = output_size or self.env.action_space.shape[0]\n",
        "        self.weights = np.zeros((self.output_size, self.input_size))\n",
        "        self.hp = hp or Hp()\n",
        "        self.normalizer = normalizer or Normalizer(self.input_size)\n",
        "        self.cur_step = 0;\n",
        "        self.set_monitor(monitor_dir)\n",
        "        self.record_video = False\n",
        "\n",
        "    def set_monitor(self, monitor_dir=None):\n",
        "        #use this method if you want to record the episode\n",
        "        #set the folder where the recorded video will be stored \n",
        "        if monitor_dir is not None:\n",
        "            should_record = lambda i: self.record_video\n",
        "            self.env = wrappers.Monitor(self.env, monitor_dir, video_callable=should_record, force=True)\n",
        "\n",
        "    def learning_rate(self, decay=0.005):\n",
        "        return self.hp.learning_rate / (1 + decay * self.cur_step)\n",
        "\n",
        "    def train(self, n_steps):\n",
        "        for step in range(n_steps):\n",
        "            self.cur_step += 1\n",
        "            # initialize the random noise deltas and the positive/negative rewards\n",
        "            deltas = self.generate_deltas()\n",
        "            positive_rewards = np.zeros(self.hp.num_deltas)\n",
        "            negative_rewards = np.zeros(self.hp.num_deltas)\n",
        "            # play an episode each with positive deltas and negative deltas, collect rewards\n",
        "            for i in range(self.hp.num_deltas):\n",
        "                positive_rewards[i] = self.play_episode(self.weights + self.hp.noise * deltas[i])\n",
        "                negative_rewards[i] = self.play_episode(self.weights - self.hp.noise * deltas[i])\n",
        "\n",
        "            # Compute the standard deviation of all rewards\n",
        "            sigma_rewards = np.array(positive_rewards + negative_rewards).std()\n",
        "\n",
        "            # Sort the rollouts by the max(r_pos, r_neg) and select the deltas with best rewards\n",
        "            scores = {k: max(r_pos, r_neg) for k, (r_pos, r_neg) in enumerate(zip(positive_rewards, negative_rewards))}\n",
        "            order = sorted(scores.keys(), key=lambda x: scores[x], reverse=True)[:self.hp.num_best_deltas]\n",
        "            rollouts = [(positive_rewards[k], negative_rewards[k], deltas[k]) for k in order]\n",
        "            # Update the policy\n",
        "            self.update_weights(rollouts, sigma_rewards)\n",
        "\n",
        "            # Only record video during evaluation, every n steps\n",
        "            if step % self.hp.record_every == 0:\n",
        "                self.record_video = True\n",
        "            # Play an episode with the new weights and print the score\n",
        "            reward_evaluation = self.play_episode(self.weights, train=False)\n",
        "            print('Step: ', step, 'Reward: ', reward_evaluation)\n",
        "            self.record_video = False\n",
        "\n",
        "    def update_weights(self, rollouts, sigma_rewards):\n",
        "        # sigma_rewards is the standard deviation of the rewards\n",
        "        step = np.zeros(self.weights.shape)\n",
        "        for r_pos, r_neg, delta in rollouts:\n",
        "            step += (r_pos - r_neg) * delta\n",
        "        self.weights += self.learning_rate() / (self.hp.num_best_deltas * sigma_rewards) * step\n",
        "\n",
        "    def play_episode(self, theta=None, train=True):\n",
        "        # play one episode of game\n",
        "        if theta is None:\n",
        "            theta = self.weights\n",
        "        obs = self.env.reset()\n",
        "        sum_reward = 0\n",
        "        episode = 0\n",
        "        while True:\n",
        "            episode += 1\n",
        "            # choose action using theta\n",
        "            action = self.predict(obs, theta)\n",
        "            obs, reward, done, _ = self.env.step(action)\n",
        "            sum_reward += reward\n",
        "            if done:\n",
        "                break\n",
        "            # break if reached max number of episodes\n",
        "            if not self.record_video and episode >= self.hp.episode_length:\n",
        "                break\n",
        "        return sum_reward\n",
        "\n",
        "    def predict(self, inp, theta):\n",
        "        # predict action from input using theta\n",
        "        inp = self.normalizer.normalize(inp)\n",
        "        return theta @ inp\n",
        "\n",
        "    def generate_deltas(self):\n",
        "        return np.random.randn(self.hp.num_deltas, *self.weights.shape)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0ka6PcmxqlML",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Train model"
      ]
    },
    {
      "metadata": {
        "id": "SuEHGTloqrYl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "hp = Hp()\n",
        "env = gym.make('BipedalWalker-v2')\n",
        "trainer = ArsTrainer(env)\n",
        "trainer.train(1000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NPbSP0vXq3rU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Train and record video and record "
      ]
    },
    {
      "metadata": {
        "id": "LLIiEOdCrI6H",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Connect to Google Drive"
      ]
    },
    {
      "metadata": {
        "id": "5BE-oZYrqhbY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "DIR_PATH = 'drive/My Drive/Colab Notebooks/BipedalWalker/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FJ1JbsL8rUoO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "create directories"
      ]
    },
    {
      "metadata": {
        "id": "KzcJ4pQusBwM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "def mkdir(base, name):\n",
        "    path = os.path.join(base, name)\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "    return path"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OruFeIvZrZMd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "videos_dir = mkdir(DIR_PATH, 'videos')\n",
        "monitor_dir = mkdir(videos_dir, 'bi')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XUIIqkznrcqd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "create display"
      ]
    },
    {
      "metadata": {
        "id": "CRkq1aT0sCza",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Start virtual display\n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1024, 768))\n",
        "display.start()\n",
        "import os\n",
        "# os.environ[\"DISPLAY\"] = \":\" + str(display.display) + \".\" + str(display.screen)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tinAJJpyrimx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "train and save video to Google Drive"
      ]
    },
    {
      "metadata": {
        "id": "Ip5C0b-iuMLS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "hp = Hp()\n",
        "env = gym.make('BipedalWalker-v2')\n",
        "trainer = ArsTrainer(env,monitor_dir=monitor_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fjoic1JvueY6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "trainer.train(20)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}